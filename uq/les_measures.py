import numpy as np
from joblib import Parallel, delayed
from functools import partial
from utils.metrics import IoU_metric, IoU_adjusted_metric, DSC_metric
import warnings
from .vox_measures import uncs_measures_names as vox_uncs_measures_names

uncs_measures_names = {
    "ddu": "DDU", "ddu_adj": "DDU$_{adj}$", "ddu_fast": "DDU$_{fast}$",
    'vvc': "VVC", 'avg_pwdice': "APWDSC", 'global_iou': "GIoU",
    "srmi": "SRMI", "smi": "SMI"

}
for vox_unc_measure in vox_uncs_measures_names.keys():
    uncs_measures_names[f"Average {vox_unc_measure}"] = "Average " + vox_uncs_measures_names[vox_unc_measure]
evaluated_uncertainty_measures = list(uncs_measures_names.keys())


def single_lesion_uncertainty(ens_seg_lab: np.ndarray, cc_label: float,
                              vox_uncs: dict, mems_seg_lab: np.ndarray,
                              mems_prob: np.ndarray, gt_lab: np.ndarray,
                              epsilon: float = 1e-5):
    """ Compute different uncertainty measures for a single connected component, that include:
    * Mean from vox uncertainty (M Dojat)
    * Detection detection uncertainty (ours)
    * Andrey's proposal: RMI avg_prod and prod_avg, MI (avg_prod and prod_avg) x (same threshold and unique thresholds)
    on the structural scale
    * Abhijit Guha Roy et al, 2019 metrics
    :param gt_lab:
    :param ens_seg_lab: predicted instance lesion mask
    :param cc_label: label in `y_pred_multi` of the lesion for which uncertainties will be computed
    :param vox_uncs: a dictionary of voxel scale uncertainty maps for different voxel measures of uncertainty, shape [H, W, D]
    :param mems_seg_lab: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param mems_prob: probabilities predicted by each model in ensemble
    :param epsilon: small value to avoid -inf in logarithms
    :return: dictionary of lesion uncertainty measures
    :rtype: dict
    """

    def structural_negated_mi(_mems_cc_masks):
        _smi = 0.
        _num_mems = _mems_cc_masks.shape[0]
        for i_m, _mem_cc_mask in enumerate(_mems_cc_masks):
            if _mem_cc_mask[_mem_cc_mask == 1].size:
                _smi += (
                            np.sum(
                                np.log(
                                    np.stack(
                                        [
                                            mems_prob[i_k][_mem_cc_mask == 1] for i_k in range(_num_mems)
                                        ], axis=0
                                    ).mean(axis=0) + epsilon
                                ) -
                                np.log(
                                    mems_prob[i_m][_mem_cc_mask == 1] + epsilon)
                            )
                        ) / np.sum(_mem_cc_mask)
        return - _smi / _num_mems

    def structural_rmi():
        _mem_cc_prob = np.asarray(
            [
                epp[cc_mask == 1] for epp in mems_prob
            ]
        )  # dim 0 - ens, dim 1 - vox
        if _mem_cc_prob.size:
            _rmi = np.log(
                _mem_cc_prob.mean(axis=0) + epsilon
            ).mean() \
                   - np.log(
                _mem_cc_prob + epsilon
            ).mean()
            return _rmi
        else:
            return 0.

    def structural_entropy():
        proba_les_k = np.asarray([epp[cc_mask == 1] for epp in mems_prob])  # dim 0 - ens, dim 1 - vox
        p1 = proba_les_k.mean(axis=1).prod()
        return - p1 * np.log(p1)
    
    def roys_measures(_mems_cc_masks):
        # average pairwise dice score
        _apdsc = np.mean(
            [
                DSC_metric(mask_i, mask_j)['DSC']
                for i_m, mask_i in enumerate(_mems_cc_masks)
                for j_m, mask_j in enumerate(_mems_cc_masks)
                if i_m > j_m
            ]
        )
        # volume variation coefficient
        _volumes = [np.sum(eccm) for eccm in _mems_cc_masks]
        _vvc = np.std(_volumes) / np.mean(_volumes)
        # global intersection over union
        _intersec = np.sum(
            (
                np.prod(_mems_cc_masks, axis=0)
            ) > 0.0
        )
        _union = np.sum(
            (
                np.sum(_mems_cc_masks, axis=0)
            ) > 0.0
        )
        return _vvc, _apdsc, _intersec / _union

    def get_max_ious_ccmasks_ens(_mems_seg_lab):
        _ious_ens_mems = []
        _mems_cc_masks = []
        for m in range(_mems_seg_lab.shape[0]):
            _iou_max = 0.0
            _mem_cc_mask_max = np.zeros_like(cc_mask)
            for _intersec_label in np.unique(cc_mask * _mems_seg_lab[m]):
                if _intersec_label != 0.0:
                    _mem_cc_mask = (_mems_seg_lab[m] == _intersec_label).astype(float)
                    _iou = IoU_metric(cc_mask, _mem_cc_mask)['IoU']
                    if _iou > _iou_max:
                        _iou_max = _iou
                        _mem_cc_mask_max = _mem_cc_mask.copy()
            _ious_ens_mems.append(_iou_max)
            _mems_cc_masks.append(_mem_cc_mask_max)
        return _ious_ens_mems, np.stack(_mems_cc_masks, axis=0)

    def get_max_gt_ious_ccmasks_ens():
        _iou_max = 0.0
        for _intersec_label in np.unique(cc_mask * gt_lab):
            if _intersec_label != 0.0:
                _mem_cc_mask = (gt_lab == _intersec_label).astype(float)
                _iou = IoU_metric(cc_mask, _mem_cc_mask)['IoU']
                if _iou > _iou_max:
                    _iou_max = _iou
        return _iou_max

    cc_mask = (ens_seg_lab == cc_label).astype(ens_seg_lab.dtype)

    ''' Check inputs '''
    if not cc_mask.shape == mems_seg_lab[0].shape == mems_prob[0].shape:
        raise ValueError(
            "Error in input dimensions:\n"
            f"cc_mask: {cc_mask.shape}, "
            f"ens_pred_prob: {mems_prob.shape}, "
            f"ens_pred_multi: {mems_seg_lab.shape}."
        )
    if not mems_prob.shape == mems_seg_lab.shape:
        raise ValueError(
            "Expecting same number of models in the ensemble:\n"
            f"ens_pred_prob: {mems_prob.shape}, "
            f"ens_pred_multi: {mems_seg_lab.shape}."
        )
    res = {'lesion label': cc_label}

    ''' Voxel-scale based uncertainties '''
    for unc_measure, vox_unc_map in vox_uncs.items():
        res.update({
            f"Average {unc_measure}": np.sum(vox_unc_map * cc_mask) / np.sum(cc_mask)
        })

    ''' Detection disagreement uncertainties '''
    ens_ious, ens_cc_masks = get_max_ious_ccmasks_ens(mems_seg_lab)
    ens_ious_adj = [
        IoU_adjusted_metric(
            cc_mask,
            y_pred=(ens_seg_lab > 0.0).astype(float),
            y_multi=ens_pred_m)['IoUadj']
        for ens_pred_m in mems_seg_lab
    ]
    ens_ious_fast = [
        IoU_metric(
            cc_mask,
            (ens_pred_m > 0.0).astype(float))['IoU']
        for ens_pred_m in mems_seg_lab
    ]
    res.update({
        'ddu': 1 - np.mean(ens_ious),
        'ddu_adj': 1 - np.mean(ens_ious_adj),
        'ddu_fast': 1 - np.mean(ens_ious_fast)
    })

    ''' Andreys measures '''
    res.update({
        'srmi': structural_rmi(),
        'smi': structural_negated_mi(ens_cc_masks),
        'seoe': structural_entropy()
    })

    ''' Roys measures '''
    res.update(dict(zip(
        ['vvc', 'avg_pwdice', 'global_iou'],
        roys_measures(ens_cc_masks)
    )))

    '''IoU with the GT'''
    res.update({
        'maxIoU': get_max_gt_ious_ccmasks_ens(),
        'IoUadj': IoU_adjusted_metric(
            cc_mask,
            y_pred=(ens_seg_lab > 0).astype(float),
            y_multi=gt_lab
        )['IoUadj'],
        'IoU': IoU_metric(
            y_pred=cc_mask,
            y=(gt_lab > 0.0).astype(float)
        )['IoU'],
        'lesion label': cc_label
    })
    return res


def lesions_uncertainty(ens_seg_lab: np.ndarray, vox_uncs: dict, mems_seg_lab: np.ndarray,
                        mems_prob: np.ndarray, gt_lab: np.ndarray, n_jobs: int = None):
    """ Parallel evaluation of all lesion scale uncertainty measures for one subject.
    :param gt_lab:
    :param ens_seg_lab: labeled lesion mask aka instance segmentation mask
    :param vox_uncs: voxel scale lesion uncertainty mask
    :param mems_seg_lab: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param mems_prob: probability maps for all models in ensemble, shape [M, H, W, D]
    :param n_jobs: number of parallel workers
    list contains uncertainty measures belonging to all lesions.
    :return:
    """

    warnings.filterwarnings("ignore")

    cc_labels = np.unique(ens_seg_lab)
    cc_labels = cc_labels[cc_labels != 0.0]

    with Parallel(n_jobs=n_jobs) as parallel_backend:
        process = partial(single_lesion_uncertainty,
                          ens_seg_lab=ens_seg_lab,
                          gt_lab=gt_lab,
                          vox_uncs=vox_uncs,
                          mems_prob=mems_prob,
                          mems_seg_lab=mems_seg_lab)
        les_uncs_list = parallel_backend(
            delayed(process)(
                cc_label=cc_label
            ) for cc_label in cc_labels
        )
    return les_uncs_list


def lesions_uncertainty_maps(y_pred_multi, vox_uncs_map, ens_pred_multi, ens_pred_prob: np.ndarray,
                             ens_pred_multi_true: np.ndarray, n_jobs: int = None):
    """ Parallel evaluation of all lesion scale uncertainty measures for one subject returned as lesion uncertainty map for each evaluated measure..
    :param y_pred_multi: labeled lesion mask aka instance segmentation mask
    :param vox_uncs_map: voxel scale lesion uncertainty mask
    :param ens_pred_multi: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param ens_pred_multi_true: labeled lesions masks generated by an ensemble of models - thresholds tuned for individual models, shape [M, H, W, D].
    :param ens_pred_prob: probability maps for all models in ensemble, shape [M, H, W, D]
    :param n_jobs: number of parallel workers
    :return: dictionary with lesion uncertainty maps for each measure
    :rtype: dict
    """
    cc_labels = np.unique(y_pred_multi)
    cc_labels = cc_labels[cc_labels != 0.0]
    '''cc_mask: np.ndarray, vox_unc_maps: dict, ens_pred_multi: np.ndarray,
                              ens_pred_multi_true: np.ndarray, ens_pred_prob'''
    with Parallel(n_jobs=n_jobs) as parallel_backend:
        process = partial(single_lesion_uncertainty,
                          y_pred_multi=y_pred_multi,
                          vox_unc_maps=vox_uncs_map,
                          ens_pred_prob=ens_pred_prob,
                          ens_pred_multi_true=ens_pred_multi_true,
                          ens_pred_multi=ens_pred_multi)
        les_uncs_list = parallel_backend(delayed(process)(
            cc_label=cc_label
        ) for cc_label in cc_labels)  # returns lists of dictionaries, but need dictionary of lists

    if les_uncs_list:
        les_uncs_measures = list(les_uncs_list[0].keys())
        results = dict(zip(les_uncs_measures, [np.zeros_like(y_pred_multi, dtype='float') for _ in les_uncs_measures]))
        # for each measure of uncertainty create a lesion uncertainty map
        for cc_label, uncs_dict in zip(cc_labels, les_uncs_list):
            for unc_name in les_uncs_measures:
                results[unc_name] += uncs_dict[unc_name] * (y_pred_multi == cc_label).astype('float')
        return results


def lesion_uncertainty_filtering(lesion_unc_measure: str, unc_threshold: float,
                                 y_pred_multi: np.ndarray, vox_unc_maps: dict = None,
                                 ens_pred_multi: np.ndarray = None, ens_pred_prob: np.ndarray = None,
                                 ens_pred_multi_true: np.ndarray = None,
                                 n_jobs: int = None):
    """
    Apply uncertainty based filtering based on the desired lesion scale uncertainty measures and a predefined threshold.
    Returns a numpy.ndarray with binary lesion masks after filtering.
    The arguments that are not used for the computation of the measure can be left None.
    :param unc_threshold: the threshold for the uncertainty
    :param lesion_unc_measure: the name of the lesion scale uncertainty measure.
    :param y_pred_multi: labeled lesion mask aka instance segmentation mask
    :param vox_unc_maps: voxel scale lesion uncertainty mask
    :param ens_pred_multi: labeled lesions masks generated by an ensemble of models - threshold tuned for ensemble, shape [M, H, W, D].
    :param ens_pred_multi_true: labeled lesions masks generated by an ensemble of models - thresholds tuned for individual models, shape [M, H, W, D].
    :param ens_pred_prob: probability maps for all models in ensemble, shape [M, H, W, D]
    :param n_jobs: number of parallel workers
    :return:
    """

    def mean_iou_det(cc_mask, ens_multi):
        ens_ious_ = []
        for m in range(ens_multi.shape[0]):
            max_iou = 0.0
            for intersec_label in np.unique(cc_mask * ens_multi[m]):
                if intersec_label != 0.0:
                    lesion_m = (ens_multi[m] == intersec_label).astype(float)
                    iou = IoU_metric(cc_mask, lesion_m)['IoU']
                    if iou > max_iou:
                        max_iou = iou
            ens_ious_.append(max_iou)
        return 1 - np.mean(ens_ious_)

    def rmi1(cc_mask, ens_prob):
        p_les = np.asarray([ep[cc_mask == 1] for ep in ens_prob])  # dim 0 - ens, dim 1 - vox
        denom = - np.log(p_les + 1e-5).mean()
        return np.log(p_les.mean(axis=0) + 1e-5).mean() + denom

    def vox_based(cc_mask, vox_uncs_maps_dict, vox_meas_name):
        return np.sum(vox_uncs_maps_dict[vox_meas_name] * cc_mask) / np.sum(cc_mask)

    cc_labels = np.unique(y_pred_multi)
    cc_labels = cc_labels[cc_labels != 0.0]

    if lesion_unc_measure == 'mean_iou_det':
        if y_pred_multi is None or ens_pred_multi is None:
            raise ValueError(
                "`y_pred_multi` and `ens_pred_multi` cannot be None for the lesion_unc_measure=`mean_iou_det`"
            )
        with Parallel(n_jobs=n_jobs) as parallel_backend:
            process = partial(mean_iou_det, ens_multi=ens_pred_multi)
            les_uncs_list = parallel_backend(delayed(process)(
                cc_mask=(y_pred_multi == cc_label).astype(y_pred_multi.dtype)
            ) for cc_label in cc_labels)
    elif lesion_unc_measure == 'mean_iou_det_true':
        if y_pred_multi is None or ens_pred_multi_true is None:
            raise ValueError(
                "`y_pred_multi` and `ens_pred_multi_true` cannot be None for the lesion_unc_measure=`mean_iou_det_true`"
            )
        with Parallel(n_jobs=n_jobs) as parallel_backend:
            process = partial(mean_iou_det, ens_multi=ens_pred_multi_true)
            les_uncs_list = parallel_backend(delayed(process)(
                cc_mask=(y_pred_multi == cc_label).astype(y_pred_multi.dtype)
            ) for cc_label in cc_labels)
    elif lesion_unc_measure == 'rmi1':
        if y_pred_multi is None or ens_pred_prob is None:
            raise ValueError(
                f"`y_pred_multi` and `ens_pred_prob` cannot be None for the lesion_unc_measure=`{lesion_unc_measure}`"
            )
        with Parallel(n_jobs=n_jobs) as parallel_backend:
            process = partial(rmi1, ens_prob=ens_pred_prob)
            les_uncs_list = parallel_backend(delayed(process)(
                cc_mask=(y_pred_multi == cc_label).astype(y_pred_multi.dtype)
            ) for cc_label in cc_labels)
    elif lesion_unc_measure.split(' ')[0] == 'mean' and lesion_unc_measure.split(' ')[
        1] in vox_uncs_measures_names.keys():
        if y_pred_multi is None or vox_unc_maps is None:
            raise ValueError(
                f"`y_pred_multi` and `vox_unc_maps` cannot be None for the lesion_unc_measure=`{lesion_unc_measure}`"
            )
        with Parallel(n_jobs=n_jobs) as parallel_backend:
            process = partial(vox_based, vox_uncs_maps_dict=vox_unc_maps,
                              vox_meas_name=lesion_unc_measure.split(' ')[1])
            les_uncs_list = parallel_backend(delayed(process)(
                cc_mask=(y_pred_multi == cc_label).astype(y_pred_multi.dtype)
            ) for cc_label in cc_labels)
    else:
        raise NotImplementedError(f"The measure {lesion_unc_measure} does not exist or is not yet implemented :/")
    les_uncs_list = np.asarray(les_uncs_list)
    cc_labels_filt = cc_labels[les_uncs_list < unc_threshold]
    return np.isin(y_pred_multi, test_elements=cc_labels_filt).astype(dtype=y_pred_multi.dtype)
